{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cell-0",
      "metadata": {
        "id": "cell-0"
      },
      "source": [
        "# Exercise 3: Mathematical Problem Solving with LLMs\n",
        "\n",
        "**This is a marked exercise (graded)**\n",
        "\n",
        "Apply LLMs to solve mathematical reasoning tasks. Test different pre-trained models with various prompting strategies and optionally fine-tune with LoRA to improve performance.\n",
        "\n",
        "**Learning Objectives:**\n",
        "- Evaluate LLMs on mathematical reasoning\n",
        "- Design effective prompts for numerical tasks\n",
        "- Implement and compare different prompting strategies\n",
        "- Optionally: Fine-tune models using LoRA\n",
        "- Measure performance using accuracy metric with tolerance\n",
        "\n",
        "**Deliverables:**\n",
        "- Completed notebook with your approach\n",
        "- `submission.csv` with predictions on test set (100 problems)\n",
        "- Score: Accuracy with 2 decimal precision tolerance (threshold: 70%)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-1",
      "metadata": {
        "id": "cell-1"
      },
      "source": [
        "## Part 1: Setup and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "cell-2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-2",
        "outputId": "659dc85a-7a0b-426f-d432-0d01b79c461b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/75.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/504.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m504.9/504.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/511.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/119.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/150.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m85.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch peft datasets pandas scikit-learn matplotlib requests -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cell-3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-3",
        "outputId": "0c704eae-3f29-4acf-8669-22ca928be7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import re\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-4",
      "metadata": {
        "id": "cell-4"
      },
      "source": [
        "## Part 2: Download Dataset\n",
        "\n",
        "Download the math problem dataset (1000 problems: 900 train, 100 test)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cell-5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-5",
        "outputId": "8423a439-4d92-47b8-ad1d-e538151212a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded train.csv\n",
            "Downloaded test.csv\n"
          ]
        }
      ],
      "source": [
        "# URLs for the dataset files\n",
        "base_url = 'https://www.raphaelcousin.com/modules/data-science-practice/module8/exercise/'\n",
        "\n",
        "train_url = base_url + 'train.csv'\n",
        "test_url = base_url + 'test.csv'\n",
        "\n",
        "def download_file(url, filename):\n",
        "    \"\"\"Download a file from URL.\"\"\"\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "    with open(filename, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f\"Downloaded {filename}\")\n",
        "\n",
        "# Download files\n",
        "download_file(train_url, 'train.csv')\n",
        "download_file(test_url, 'test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cell-6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-6",
        "outputId": "b31232db-b8ed-48fe-b181-be69345782e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set size: 900\n",
            "Test set size: 100\n",
            "\n",
            "Training set category distribution:\n",
            "category\n",
            "algebra          150\n",
            "arithmetic       153\n",
            "fractions        143\n",
            "geometry         155\n",
            "percentage       152\n",
            "word_problems    147\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sample training problems:\n",
            "   id       category                                            problem  \\\n",
            "0   0     percentage                                Increase 109 by 25%   \n",
            "1   1     arithmetic                                   What is 76 + 55?   \n",
            "2   2  word_problems  Sarah has $286. She spends $128. How much mone...   \n",
            "3   3       geometry  What is the circumference of a circle with rad...   \n",
            "4   4       geometry   What is the volume of a cube with side length 3?   \n",
            "5   5     percentage                                 What is 7% of 132?   \n",
            "6   6  word_problems  John is 10 years old now. How old was he 15 ye...   \n",
            "7   7      fractions                  What is 1/5 + 2/5? (decimal form)   \n",
            "8   8     percentage                                What is 27% of 164?   \n",
            "9   9      fractions                  What is 2/4 + 1/4? (decimal form)   \n",
            "\n",
            "   solution  \n",
            "0    136.25  \n",
            "1    131.00  \n",
            "2    158.00  \n",
            "3     87.92  \n",
            "4     27.00  \n",
            "5      9.24  \n",
            "6     -5.00  \n",
            "7      0.60  \n",
            "8     44.28  \n",
            "9      0.75  \n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "\n",
        "print(f\"Train set size: {len(train_data)}\")\n",
        "print(f\"Test set size: {len(test_data)}\")\n",
        "\n",
        "# Display category distribution\n",
        "print(\"\\nTraining set category distribution:\")\n",
        "print(train_data['category'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nSample training problems:\")\n",
        "print(train_data.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-7",
      "metadata": {
        "id": "cell-7"
      },
      "source": [
        "## Part 3: Baseline - Dummy Model\n",
        "\n",
        "Create a baseline to understand what poor performance looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "cell-8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-8",
        "outputId": "9fa50102-3ba5-4f0f-d2b1-52a530959044"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy model (always predicts mean): 150.79\n",
            "This demonstrates very poor performance. Your model should do much better!\n"
          ]
        }
      ],
      "source": [
        "def check_accuracy(predictions, ground_truth, tolerance=0.01):\n",
        "    \"\"\"\n",
        "    Calculate accuracy with tolerance for floating point comparisons.\n",
        "\n",
        "    Two values are considered equal if their difference is <= tolerance\n",
        "    OR if they round to the same value at 2 decimal places.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    for pred, truth in zip(predictions, ground_truth):\n",
        "        # Check if both round to same 2 decimal places\n",
        "        if round(pred, 2) == round(truth, 2):\n",
        "            correct += 1\n",
        "        # Or if absolute difference is very small\n",
        "        elif abs(pred - truth) <= tolerance:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(predictions)\n",
        "\n",
        "# Dummy baseline: always predict the mean\n",
        "mean_solution = train_data['solution'].mean()\n",
        "print(f\"Dummy model (always predicts mean): {mean_solution:.2f}\")\n",
        "print(\"This demonstrates very poor performance. Your model should do much better!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-9",
      "metadata": {
        "id": "cell-9"
      },
      "source": [
        "## Part 4: Utility Functions\n",
        "\n",
        "Helper functions to extract numerical answers from model outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cell-10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-10",
        "outputId": "194945e5-2379-4189-fdf3-ac5663f3a5ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number extraction tests:\n",
            "  'The answer is 42' -> 42.0\n",
            "  '42' -> 42.0\n",
            "  '15 + 27 = 42' -> 42.0\n",
            "  'Calculating... the result is 42.5!' -> 42.5\n",
            "  'No number here' -> None\n",
            "  'The value is -15' -> -15.0\n"
          ]
        }
      ],
      "source": [
        "def extract_number(text):\n",
        "    \"\"\"\n",
        "    Extract the first number from text. Return None if no number found.\n",
        "\n",
        "    Handles various formats:\n",
        "    - \"The answer is 42\"\n",
        "    - \"42\"\n",
        "    - \"= 42\"\n",
        "    - \"Result: 42.5\"\n",
        "    - Negative numbers: \"-15\"\n",
        "    \"\"\"\n",
        "    # Try different patterns in order of specificity\n",
        "    patterns = [\n",
        "        r'(?:answer|result|equals?|=)\\s*:?\\s*(-?\\d+\\.?\\d*)',  # \"answer is 42\" or \"= 42\"\n",
        "        r'(-?\\d+\\.?\\d*)\\s*$',  # Number at the end\n",
        "        r'(-?\\d+\\.?\\d*)',  # Any number\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, text, re.IGNORECASE)\n",
        "        if match:\n",
        "            try:\n",
        "                return float(match.group(1))\n",
        "            except (ValueError, IndexError):\n",
        "                continue\n",
        "\n",
        "    return None\n",
        "\n",
        "# Test extraction\n",
        "test_strings = [\n",
        "    \"The answer is 42\",\n",
        "    \"42\",\n",
        "    \"15 + 27 = 42\",\n",
        "    \"Calculating... the result is 42.5!\",\n",
        "    \"No number here\",\n",
        "    \"The value is -15\"\n",
        "]\n",
        "\n",
        "print(\"Number extraction tests:\")\n",
        "for s in test_strings:\n",
        "    result = extract_number(s)\n",
        "    print(f\"  '{s}' -> {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-11",
      "metadata": {
        "id": "cell-11"
      },
      "source": [
        "## Part 5: Load Pre-trained Model\n",
        "\n",
        "Load a small, efficient model for math problem solving."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-12",
        "outputId": "a633863f-3695-4d69-a719-6a9e4cc8113a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu üöÄ\n",
            "Loading model: Qwen/Qwen1.5-1.8B...\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# 1. D√©finition de la variable 'device' (essentiel pour corriger le NameError)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device} üöÄ\")\n",
        "\n",
        "# 2. Chargement du Mod√®le Qwen\n",
        "model_name = \"Qwen/Qwen1.5-1.8B\"\n",
        "\n",
        "print(f\"Loading model: {model_name}...\")\n",
        "# Le tokenizer Qwen n√©cessite trust_remote_code=True\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n",
        "# 3. D√©placement du mod√®le vers le dispositif\n",
        "model = model.to(device)\n",
        "\n",
        "# D√©finir le token de padding\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 4. Initialisation du pipeline de g√©n√©ration\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=model.device.index if model.device.type == 'cuda' else -1\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully! ‚úÖ\")\n",
        "print(f\"Model size: {sum(p.numel() for p in model.parameters()) / 1e6:.1f}M parameters\")\n",
        "print(\"Pipeline 'generator' est pr√™t √† l'emploi.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-13",
      "metadata": {
        "id": "cell-13"
      },
      "source": [
        "## Part 6: Prompting Strategies\n",
        "\n",
        "Test different prompt templates to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-14",
      "metadata": {
        "id": "cell-14"
      },
      "outputs": [],
      "source": [
        "def generate_answer(problem, prompt_template=\"simple\", max_new_tokens=50, temperature=0.1):\n",
        "    \"\"\"\n",
        "    Generate answer using different prompt templates.\n",
        "\n",
        "    Templates:\n",
        "    - simple: Just the problem\n",
        "    - instruction: Add instruction to solve\n",
        "    - cot: Chain-of-thought prompting\n",
        "    - few_shot: Include examples from training data\n",
        "    \"\"\"\n",
        "    if prompt_template == \"simple\":\n",
        "        prompt = f\"{problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"instruction\":\n",
        "        prompt = f\"Solve this math problem and provide only the numerical answer.\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    elif prompt_template == \"cot\":\n",
        "        prompt = f\"Solve this math problem step by step, then provide the final numerical answer.\\n\\nProblem: {problem}\\nSolution:\\n\"\n",
        "\n",
        "    elif prompt_template == \"few_shot\":\n",
        "        # Include 3-5 examples from training data\n",
        "        examples = []\n",
        "        for i in range(min(5, len(train_data))):\n",
        "            examples.append(f\"Problem: {train_data['problem'].iloc[i]}\\nAnswer: {train_data['solution'].iloc[i]}\")\n",
        "\n",
        "        examples_text = \"\\n\\n\".join(examples)\n",
        "        prompt = f\"{examples_text}\\n\\nProblem: {problem}\\nAnswer:\"\n",
        "\n",
        "    else:\n",
        "        prompt = problem\n",
        "\n",
        "    # Generate\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "            do_sample=True if temperature > 0 else False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt from response\n",
        "    response = response[len(prompt):].strip()\n",
        "\n",
        "    return response\n",
        "\n",
        "# Test different prompts on a sample problem\n",
        "test_problem = train_data['problem'].iloc[0]\n",
        "test_solution = train_data['solution'].iloc[0]\n",
        "\n",
        "print(f\"Testing problem: {test_problem}\")\n",
        "print(f\"Correct answer: {test_solution}\\n\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for template in [\"simple\", \"instruction\", \"cot\", \"few_shot\"]:\n",
        "    response = generate_answer(test_problem, template)\n",
        "    extracted = extract_number(response)\n",
        "\n",
        "    correct = \"‚úì\" if extracted is not None and round(extracted, 2) == round(test_solution, 2) else \"‚úó\"\n",
        "\n",
        "    print(f\"{correct} {template}:\")\n",
        "    print(f\"  Response: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
        "    print(f\"  Extracted: {extracted}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VmRt0_h73Cdf"
      },
      "id": "VmRt0_h73Cdf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "cell-15",
      "metadata": {
        "id": "cell-15"
      },
      "source": [
        "## Part 7: Evaluate on Validation Set\n",
        "\n",
        "Test your best prompting strategy on a subset of training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cell-16",
        "outputId": "41518976-15ef-4354-c831-d526b506224b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on 50 validation problems using few_shot prompting...\n",
            "\n",
            "Processed 10/50 problems...\n",
            "Processed 20/50 problems...\n"
          ]
        }
      ],
      "source": [
        "best_template = \"few_shot\"\n",
        "\n",
        "val_data = train_data.tail(50).copy()\n",
        "\n",
        "predictions = []\n",
        "ground_truth = val_data['solution'].tolist()\n",
        "\n",
        "print(f\"Evaluating on {len(val_data)} validation problems using {best_template} prompting...\\n\")\n",
        "\n",
        "for idx, row in val_data.iterrows():\n",
        "    problem = row['problem']\n",
        "\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "\n",
        "    predictions.append(prediction)\n",
        "\n",
        "    if (len(predictions) % 10) == 0:\n",
        "        print(f\"Processed {len(predictions)}/{len(val_data)} problems...\")\n",
        "\n",
        "accuracy = check_accuracy(predictions, ground_truth)\n",
        "print(f\"\\nValidation Accuracy ({best_template}): **{accuracy:.2%}**\")\n",
        "print(f\"Need to achieve: 70% on test set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-17",
      "metadata": {
        "id": "cell-17"
      },
      "source": [
        "## Part 8: Generate Test Predictions\n",
        "\n",
        "Generate predictions for the test set and create submission file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-18",
      "metadata": {
        "id": "cell-18"
      },
      "outputs": [],
      "source": [
        "best_template = \"few_shot\"\n",
        "\n",
        "print(f\"Generating predictions on {len(test_data)} test problems using {best_template} prompting...\\n\")\n",
        "\n",
        "test_predictions = []\n",
        "test_ground_truth = test_data['solution'].tolist()\n",
        "\n",
        "for idx, row in test_data.iterrows():\n",
        "    problem = row['problem']\n",
        "\n",
        "    response = generate_answer(problem, prompt_template=best_template)\n",
        "    prediction = extract_number(response)\n",
        "\n",
        "    if prediction is None:\n",
        "        prediction = 0.0\n",
        "        print(f\"‚ö†Ô∏è Warning: No number extracted for problem {idx}: {problem[:50]}...\")\n",
        "\n",
        "    test_predictions.append(prediction)\n",
        "\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"Processed {idx + 1}/{len(test_data)} problems...\")\n",
        "\n",
        "print(\"\\nAll test predictions generated!\")\n",
        "\n",
        "print(\"\\n--- Final Evaluation ---\")\n",
        "final_accuracy = check_accuracy(test_predictions, test_ground_truth)\n",
        "\n",
        "print(f\"Final Test Accuracy ({best_template}): **{final_accuracy:.2%}**\")\n",
        "print(f\"Goal: 70%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-19",
      "metadata": {
        "id": "cell-19"
      },
      "source": [
        "## Part 9: Create Submission File\n",
        "\n",
        "Save predictions in the required format for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-20",
      "metadata": {
        "id": "cell-20"
      },
      "outputs": [],
      "source": [
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'id': test_data['id'],\n",
        "    'solution': test_predictions\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Submission file created: submission.csv\")\n",
        "print(\"\\nSubmission preview:\")\n",
        "print(submission.head(10))\n",
        "\n",
        "# Verify all predictions are numerical\n",
        "non_numeric = submission['solution'].isna().sum()\n",
        "if non_numeric > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: {non_numeric} predictions are not numerical!\")\n",
        "    print(\"These will result in incorrect answers. Please fix them.\")\n",
        "else:\n",
        "    print(\"\\n‚úì All predictions are numerical\")\n",
        "\n",
        "# Show statistics\n",
        "print(\"\\nPrediction statistics:\")\n",
        "print(submission['solution'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-21",
      "metadata": {
        "id": "cell-21"
      },
      "source": [
        "## Part 10 (Optional): Fine-Tuning with LoRA\n",
        "\n",
        "If prompting doesn't achieve 70% accuracy, consider fine-tuning with LoRA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cell-22",
      "metadata": {
        "id": "cell-22"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement LoRA fine-tuning (OPTIONAL)\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# This is a template - implement if needed\n",
        "print(\"LoRA fine-tuning is optional.\")\n",
        "print(\"Use this if prompting strategies don't achieve 70% accuracy.\")\n",
        "print(\"\\nConsider:\")\n",
        "print(\"- Prepare training dataset in correct format\")\n",
        "print(\"- Configure LoRA parameters (r=8, alpha=32)\")\n",
        "print(\"- Train for a few epochs\")\n",
        "print(\"- Evaluate and compare with prompting approaches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cell-23",
      "metadata": {
        "id": "cell-23"
      },
      "source": [
        "## Questions\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. **Which prompting strategy worked best and why?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "2. **What types of math problems were most challenging for the model?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "3. **How did you handle number extraction from model outputs?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "4. **What are the limitations of using LLMs for mathematical reasoning?**\n",
        "   - YOUR ANSWER HERE\n",
        "\n",
        "5. **If you used LoRA fine-tuning, what were the trade-offs compared to prompting?**\n",
        "   - YOUR ANSWER HERE (or N/A if you didn't use LoRA)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}