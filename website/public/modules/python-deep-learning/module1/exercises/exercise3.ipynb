{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIGlckmy2rFA"
      },
      "source": [
        "# Module 1 - Exercise 3: First Step with MLP\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the structure of nn.Linear layers (input and output dimensions)\n",
        "- Learn how to use basic activation functions (ReLU, Sigmoid, Tanh)\n",
        "- Build simple neural networks using nn.Sequential\n",
        "- Calculate the number of parameters in a neural network\n",
        "- Perform forward pass operations through the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJTSQP1T2rFB"
      },
      "source": [
        "## Test Framework Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wR5MqrhU2rFC"
      },
      "outputs": [],
      "source": [
        "# Clone the test repository\n",
        "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
        "\n",
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
        "\n",
        "# Import the improved test utilities\n",
        "from test_utils import NotebookTestRunner, create_inline_test\n",
        "from module1.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
        "\n",
        "# Create test runner and validator\n",
        "test_runner = NotebookTestRunner(\"module1\", 3)\n",
        "validator = Exercise3Validator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlyWJHj82rFC"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fg82BCy2rFC",
        "outputId": "d5270789-fa20-4031-df96-8645d625a772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doxWT1AF2rFC"
      },
      "source": [
        "## Section 1: Understanding nn.Linear\n",
        "\n",
        "The `nn.Linear` layer is the fundamental building block of MLPs. It performs a linear transformation: `y = xW^T + b`\n",
        "where x is the input, W is the weight matrix, and b is the bias vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbUXpAD02rFC",
        "outputId": "75265e78-f557-4bfd-81d2-f9cee054a8d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Weight shape: torch.Size([5, 10])\n",
            "Bias shape: torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms input from 10 features to 5 features\n",
        "linear_layer_1 = nn.Linear(10,5)\n",
        "\n",
        "# Display layer information\n",
        "if linear_layer_1 is not None:\n",
        "    print(f\"Linear layer: {linear_layer_1}\")\n",
        "    print(f\"Weight shape: {linear_layer_1.weight.shape}\")\n",
        "    print(f\"Bias shape: {linear_layer_1.bias.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKlw4rt2rFD",
        "outputId": "b33a46c2-f362-4eef-a387-75f02d1c27ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer 2: Linear(in_features=5, out_features=3, bias=True)\n",
            "Calculated parameters: 18\n",
            "Actual parameters: 18\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms 5 features to 3 features\n",
        "linear_layer_2 = nn.Linear(5,3)\n",
        "\n",
        "# TODO: Calculate the total number of parameters in linear_layer_2\n",
        "# Remember: parameters = (input_size * output_size) + bias_size\n",
        "num_params_layer2 = torch.tensor(linear_layer_2.weight.shape).prod() + torch.tensor(linear_layer_2.bias.shape).prod()\n",
        "\n",
        "if linear_layer_2 is not None and num_params_layer2 is not None:\n",
        "    print(f\"Linear layer 2: {linear_layer_2}\")\n",
        "    print(f\"Calculated parameters: {num_params_layer2}\")\n",
        "    actual_params = sum(p.numel() for p in linear_layer_2.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKQGMF92rFD",
        "outputId": "a9844027-2b27-4b15-ead2-8130c9d2793b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 1: Understanding nn.Linear\n",
            "==================================================\n",
            "❌ Linear layer with 10 inputs and 5 outputs: Unexpected error: Exercise3Validator.test_linear_layer_1() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer with 5 inputs and 3 outputs: Unexpected error: Exercise3Validator.test_linear_layer_2() missing 1 required positional argument: 'variables'\n",
            "❌ Correct parameter count for linear_layer_2: Unexpected error: Exercise3Validator.test_num_params_layer2() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 1: Understanding nn.Linear - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Test Section 1: Understanding nn.Linear\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Understanding nn.Linear\"]]\n",
        "test_runner.test_section(\"Section 1: Understanding nn.Linear\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMyiXF0X2rFD"
      },
      "source": [
        "## Section 2: Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
        "- **ReLU**: f(x) = max(0, x) - Most commonly used\n",
        "- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
        "- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpCgy-5q2rFD",
        "outputId": "96cd97dc-5a6b-41b5-9b1d-160ed59fdf01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([-2., -1.,  0.,  1.,  2.])\n",
            "ReLU output: tensor([0., 0., 0., 1., 2.])\n",
            "Sigmoid output: tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
            "Tanh output: tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create instances of the three main activation functions\n",
        "relu_activation = nn.ReLU()\n",
        "sigmoid_activation = nn.Sigmoid()\n",
        "tanh_activation = nn.Tanh()\n",
        "\n",
        "# Test the activations with sample input\n",
        "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "\n",
        "if relu_activation is not None:\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"ReLU output: {relu_activation(test_input)}\")\n",
        "if sigmoid_activation is not None:\n",
        "    print(f\"Sigmoid output: {sigmoid_activation(test_input)}\")\n",
        "if tanh_activation is not None:\n",
        "    print(f\"Tanh output: {tanh_activation(test_input)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtMb-jdE2rFD",
        "outputId": "53873f1e-e2b4-467e-eebf-b60bdf0e1a5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 10])\n",
            "Linear output shape: torch.Size([2, 5])\n",
            "Activated output shape: torch.Size([2, 5])\n",
            "Number of negative values before ReLU: 2\n",
            "Number of negative values after ReLU: 0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Apply ReLU activation to the output of linear_layer_1\n",
        "# First create some input data\n",
        "input_data = torch.randn(2, 10)  # Batch size 2, 10 features\n",
        "\n",
        "# TODO: Pass input_data through linear_layer_1\n",
        "linear_output = linear_layer_1(input_data)\n",
        "\n",
        "# TODO: Apply ReLU activation to linear_output\n",
        "activated_output = relu_activation(linear_output)\n",
        "\n",
        "if linear_output is not None and activated_output is not None:\n",
        "    print(f\"Input shape: {input_data.shape}\")\n",
        "    print(f\"Linear output shape: {linear_output.shape}\")\n",
        "    print(f\"Activated output shape: {activated_output.shape}\")\n",
        "    print(f\"Number of negative values before ReLU: {(linear_output < 0).sum().item()}\")\n",
        "    print(f\"Number of negative values after ReLU: {(activated_output < 0).sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjrTFja2rFD",
        "outputId": "204063a5-4660-4994-8dc9-0e8af8aff3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 2: Activation Functions\n",
            "==================================================\n",
            "❌ ReLU activation function: Unexpected error: Exercise3Validator.test_relu_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Sigmoid activation function: Unexpected error: Exercise3Validator.test_sigmoid_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Tanh activation function: Unexpected error: Exercise3Validator.test_tanh_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer output computation: Unexpected error: Exercise3Validator.test_linear_output() missing 1 required positional argument: 'variables'\n",
            "❌ ReLU activation applied correctly: Unexpected error: Exercise3Validator.test_activated_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 2: Activation Functions - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Test Section 2: Activation Functions\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Activation Functions\"]]\n",
        "test_runner.test_section(\"Section 2: Activation Functions\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geRw91N92rFD"
      },
      "source": [
        "## Section 3: Building Networks with nn.Sequential\n",
        "\n",
        "`nn.Sequential` allows us to stack layers and create a neural network pipeline. The output of each layer becomes the input to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTrAyrl32rFD",
        "outputId": "4ba51455-01d1-4221-e0a9-33a2f7431444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple MLP architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=5, bias=True)\n",
            "  (1): ReLU()\n",
            ")\n",
            "\n",
            "Total parameters: 55\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a simple 2-layer MLP using nn.Sequential\n",
        "# Input: 8 features -> Hidden: 4 neurons with ReLU -> Output: 2 neurons\n",
        "simple_mlp = nn.Sequential(\n",
        "    nn.Linear(10, 5),\n",
        "    nn.ReLU()\n",
        ")\n",
        "\n",
        "if simple_mlp is not None:\n",
        "    print(\"Simple MLP architecture:\")\n",
        "    print(simple_mlp)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jY6mkeSt2rFE"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a deeper MLP with 3 hidden layers\n",
        "# Input: 10 -> Hidden1: 8 (ReLU) -> Hidden2: 6 (ReLU) -> Hidden3: 4 (ReLU) -> Output: 2\n",
        "deep_mlp = None\n",
        "# TODO: Calculate the total number of parameters in deep_mlp\n",
        "# Parameters per layer: (input_size * output_size) + output_size\n",
        "# Layer 1: (10 * 8) + 8 = 88\n",
        "# Layer 2: (8 * 6) + 6 = 54\n",
        "# Layer 3: (6 * 4) + 4 = 28\n",
        "# Layer 4: (4 * 2) + 2 = 10\n",
        "deep_mlp_params = None  # Calculate the sum\n",
        "\n",
        "if deep_mlp is not None and deep_mlp_params is not None:\n",
        "    print(\"Deep MLP architecture:\")\n",
        "    print(deep_mlp)\n",
        "    print(f\"\\nCalculated parameters: {deep_mlp_params}\")\n",
        "    actual_params = sum(p.numel() for p in deep_mlp.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqNitb3D2rFE",
        "outputId": "76d1522b-7cfb-4740-f61d-2b042bd57cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 3: Building Networks with nn.Sequential\n",
            "==================================================\n",
            "❌ Simple 2-layer MLP structure: Unexpected error: Exercise3Validator.test_simple_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Deep 4-layer MLP structure: Unexpected error: Exercise3Validator.test_deep_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Correct parameter count for deep_mlp: Unexpected error: Exercise3Validator.test_deep_mlp_params() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 3: Building Networks with nn.Sequential - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Test Section 3: Building Networks with nn.Sequential\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Building Networks with nn.Sequential\"]]\n",
        "test_runner.test_section(\"Section 3: Building Networks with nn.Sequential\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9LcSpfU2rFE"
      },
      "source": [
        "## Section 4: Forward Pass\n",
        "\n",
        "The forward pass is the process of passing input data through the network to get predictions. Each layer transforms the data sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "bDD6XZ9s2rFE"
      },
      "outputs": [],
      "source": [
        "# TODO: Perform a forward pass through simple_mlp\n",
        "# Create input data with batch size 3 and 8 features\n",
        "forward_input = torch.randn(3, 8)\n",
        "\n",
        "# TODO: Pass the input through simple_mlp\n",
        "simple_output = None\n",
        "\n",
        "if simple_output is not None:\n",
        "    print(f\"Input shape: {forward_input.shape}\")\n",
        "    print(f\"Output shape: {simple_output.shape}\")\n",
        "    print(f\"Output values:\\n{simple_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kHFDRT-u2rFE"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a network with mixed activation functions\n",
        "# Input: 6 -> Hidden1: 4 (ReLU) -> Hidden2: 3 (Tanh) -> Output: 1 (Sigmoid)\n",
        "mixed_activation_mlp = None\n",
        "\n",
        "# TODO: Perform forward pass with batch size 5\n",
        "mixed_input = torch.randn(5, 6)\n",
        "mixed_output = None  # Pass mixed_input through mixed_activation_mlp\n",
        "\n",
        "if mixed_activation_mlp is not None and mixed_output is not None:\n",
        "    print(\"Mixed activation MLP:\")\n",
        "    print(mixed_activation_mlp)\n",
        "    print(f\"\\nInput shape: {mixed_input.shape}\")\n",
        "    print(f\"Output shape: {mixed_output.shape}\")\n",
        "    print(f\"Output range: [{mixed_output.min().item():.4f}, {mixed_output.max().item():.4f}]\")\n",
        "    print(\"(Note: Sigmoid ensures output is between 0 and 1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4I0RPPb2rFE",
        "outputId": "b3aa53db-64b5-4fcb-8180-e7fb59dcc40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 4: Forward Pass\n",
            "==================================================\n",
            "❌ Forward pass through simple_mlp: Unexpected error: Exercise3Validator.test_simple_output() missing 1 required positional argument: 'variables'\n",
            "❌ MLP with mixed activations: Unexpected error: Exercise3Validator.test_mixed_activation_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Forward pass through mixed_activation_mlp: Unexpected error: Exercise3Validator.test_mixed_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 4: Forward Pass - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Test Section 4: Forward Pass\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Forward Pass\"]]\n",
        "test_runner.test_section(\"Section 4: Forward Pass\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW6VtJjp2rFE"
      },
      "source": [
        "## Section 5: Understanding Parameter Counting\n",
        "\n",
        "Understanding how many parameters your network has is crucial for model complexity and memory requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UaLIfqMO2rFE"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a function to count parameters in any model\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count the total number of trainable parameters in a model.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch nn.Module\n",
        "\n",
        "    Returns:\n",
        "        Total number of parameters\n",
        "    \"\"\"\n",
        "    # TODO: Complete this function\n",
        "    return None\n",
        "\n",
        "# Test your function\n",
        "if count_parameters is not None and simple_mlp is not None:\n",
        "    param_count = count_parameters(simple_mlp)\n",
        "    if param_count is not None:\n",
        "        print(f\"Simple MLP parameters: {param_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "k7fGPtZZ2rFF"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a large MLP and calculate its parameters\n",
        "# Input: 100 -> Hidden1: 64 -> Hidden2: 32 -> Hidden3: 16 -> Output: 10\n",
        "# Use ReLU activation between layers (except after output)\n",
        "large_mlp = None\n",
        "\n",
        "# TODO: Calculate expected number of parameters manually\n",
        "# Layer 1: (100 * 64) + 64 = ?\n",
        "# Layer 2: (64 * 32) + 32 = ?\n",
        "# Layer 3: (32 * 16) + 16 = ?\n",
        "# Layer 4: (16 * 10) + 10 = ?\n",
        "expected_params = None  # Sum all layer parameters\n",
        "\n",
        "if large_mlp is not None and expected_params is not None and count_parameters is not None:\n",
        "    actual_params = count_parameters(large_mlp)\n",
        "    if actual_params is not None:\n",
        "        print(f\"Expected parameters: {expected_params}\")\n",
        "        print(f\"Actual parameters: {actual_params}\")\n",
        "        print(f\"Match: {expected_params == actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sr_Bnoe2rFF",
        "outputId": "c835687e-ad67-45ef-8f16-c1a7064cc02b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 5: Understanding Parameter Counting\n",
            "==================================================\n",
            "❌ Parameter counting function: Unexpected error: Exercise3Validator.test_count_parameters_function() missing 1 required positional argument: 'variables'\n",
            "❌ Large MLP structure: Unexpected error: Exercise3Validator.test_large_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Correct manual parameter calculation: Unexpected error: Exercise3Validator.test_expected_params() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 5: Understanding Parameter Counting - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Test Section 5: Understanding Parameter Counting\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Understanding Parameter Counting\"]]\n",
        "test_runner.test_section(\"Section 5: Understanding Parameter Counting\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APJJavHm2rFF"
      },
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6AvJNen2rFF",
        "outputId": "30fcd518-5901-4b26-a31a-0da5c0292c90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL TEST SUMMARY\n",
            "============================================================\n",
            "❌ Section 1: Understanding nn.Linear: 0/3 tests passed\n",
            "❌ Section 2: Activation Functions: 0/5 tests passed\n",
            "❌ Section 3: Building Networks with nn.Sequential: 0/3 tests passed\n",
            "❌ Section 4: Forward Pass: 0/3 tests passed\n",
            "❌ Section 5: Understanding Parameter Counting: 0/3 tests passed\n",
            "============================================================\n",
            "❌ Some tests are still failing. Please review and complete the TODOs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Display final summary of all tests\n",
        "test_runner.final_summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}