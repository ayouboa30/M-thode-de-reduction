{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIGlckmy2rFA"
      },
      "source": [
        "# Module 1 - Exercise 3: First Step with MLP\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand the structure of nn.Linear layers (input and output dimensions)\n",
        "- Learn how to use basic activation functions (ReLU, Sigmoid, Tanh)\n",
        "- Build simple neural networks using nn.Sequential\n",
        "- Calculate the number of parameters in a neural network\n",
        "- Perform forward pass operations through the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJTSQP1T2rFB"
      },
      "source": [
        "## Test Framework Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wR5MqrhU2rFC"
      },
      "outputs": [],
      "source": [
        "# Clone the test repository\n",
        "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
        "\n",
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
        "\n",
        "# Import the improved test utilities\n",
        "from test_utils import NotebookTestRunner, create_inline_test\n",
        "from module1.test_exercise3 import Exercise3Validator, EXERCISE3_SECTIONS\n",
        "\n",
        "# Create test runner and validator\n",
        "test_runner = NotebookTestRunner(\"module1\", 3)\n",
        "validator = Exercise3Validator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlyWJHj82rFC"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fg82BCy2rFC",
        "outputId": "682e5deb-d26d-4cbf-d6fd-42a1b6000ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doxWT1AF2rFC"
      },
      "source": [
        "## Section 1: Understanding nn.Linear\n",
        "\n",
        "The `nn.Linear` layer is the fundamental building block of MLPs. It performs a linear transformation: `y = xW^T + b`\n",
        "where x is the input, W is the weight matrix, and b is the bias vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbUXpAD02rFC",
        "outputId": "d8cc0c8b-66b1-4204-d4f6-e14803129290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer: Linear(in_features=10, out_features=5, bias=True)\n",
            "Weight shape: torch.Size([5, 10])\n",
            "Bias shape: torch.Size([5])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms input from 10 features to 5 features\n",
        "linear_layer_1 = nn.Linear(10,5)\n",
        "\n",
        "# Display layer information\n",
        "if linear_layer_1 is not None:\n",
        "    print(f\"Linear layer: {linear_layer_1}\")\n",
        "    print(f\"Weight shape: {linear_layer_1.weight.shape}\")\n",
        "    print(f\"Bias shape: {linear_layer_1.bias.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HKlw4rt2rFD",
        "outputId": "7ed69ce0-9d88-4e90-90d7-175cee3d1255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear layer 2: Linear(in_features=5, out_features=3, bias=True)\n",
            "Calculated parameters: 18\n",
            "Actual parameters: 18\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a linear layer that transforms 5 features to 3 features\n",
        "linear_layer_2 = nn.Linear(5,3)\n",
        "\n",
        "# TODO: Calculate the total number of parameters in linear_layer_2\n",
        "# Remember: parameters = (input_size * output_size) + bias_size\n",
        "num_params_layer2 = torch.tensor(linear_layer_2.weight.shape).prod() + torch.tensor(linear_layer_2.bias.shape).prod()\n",
        "\n",
        "if linear_layer_2 is not None and num_params_layer2 is not None:\n",
        "    print(f\"Linear layer 2: {linear_layer_2}\")\n",
        "    print(f\"Calculated parameters: {num_params_layer2}\")\n",
        "    actual_params = sum(p.numel() for p in linear_layer_2.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePKQGMF92rFD",
        "outputId": "03e21a62-e9df-46a5-9b60-451268591524"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 1: Understanding nn.Linear\n",
            "==================================================\n",
            "❌ Linear layer with 10 inputs and 5 outputs: Unexpected error: Exercise3Validator.test_linear_layer_1() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer with 5 inputs and 3 outputs: Unexpected error: Exercise3Validator.test_linear_layer_2() missing 1 required positional argument: 'variables'\n",
            "❌ Correct parameter count for linear_layer_2: Unexpected error: Exercise3Validator.test_num_params_layer2() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 1: Understanding nn.Linear - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# Test Section 1: Understanding nn.Linear\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 1: Understanding nn.Linear\"]]\n",
        "test_runner.test_section(\"Section 1: Understanding nn.Linear\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMyiXF0X2rFD"
      },
      "source": [
        "## Section 2: Activation Functions\n",
        "\n",
        "Activation functions introduce non-linearity into neural networks, allowing them to learn complex patterns.\n",
        "- **ReLU**: f(x) = max(0, x) - Most commonly used\n",
        "- **Sigmoid**: f(x) = 1/(1+e^(-x)) - Outputs between 0 and 1\n",
        "- **Tanh**: f(x) = (e^x - e^(-x))/(e^x + e^(-x)) - Outputs between -1 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpCgy-5q2rFD",
        "outputId": "00fde152-4150-439a-fb06-9e9cb42ed3c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: tensor([-2., -1.,  0.,  1.,  2.])\n",
            "ReLU output: tensor([0., 0., 0., 1., 2.])\n",
            "Sigmoid output: tensor([0.1192, 0.2689, 0.5000, 0.7311, 0.8808])\n",
            "Tanh output: tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create instances of the three main activation functions\n",
        "relu_activation = nn.ReLU()\n",
        "sigmoid_activation = nn.Sigmoid()\n",
        "tanh_activation = nn.Tanh()\n",
        "\n",
        "# Test the activations with sample input\n",
        "test_input = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "\n",
        "if relu_activation is not None:\n",
        "    print(f\"Input: {test_input}\")\n",
        "    print(f\"ReLU output: {relu_activation(test_input)}\")\n",
        "if sigmoid_activation is not None:\n",
        "    print(f\"Sigmoid output: {sigmoid_activation(test_input)}\")\n",
        "if tanh_activation is not None:\n",
        "    print(f\"Tanh output: {tanh_activation(test_input)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtMb-jdE2rFD",
        "outputId": "613e6dba-3324-4049-bf32-7cbbc9718784"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 10])\n",
            "Linear output shape: torch.Size([2, 5])\n",
            "Activated output shape: torch.Size([2, 5])\n",
            "Number of negative values before ReLU: 8\n",
            "Number of negative values after ReLU: 0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Apply ReLU activation to the output of linear_layer_1\n",
        "# First create some input data\n",
        "input_data = torch.randn(2, 10)  # Batch size 2, 10 features\n",
        "\n",
        "# TODO: Pass input_data through linear_layer_1\n",
        "linear_output = linear_layer_1(input_data)\n",
        "\n",
        "# TODO: Apply ReLU activation to linear_output\n",
        "activated_output = relu_activation(linear_output)\n",
        "\n",
        "if linear_output is not None and activated_output is not None:\n",
        "    print(f\"Input shape: {input_data.shape}\")\n",
        "    print(f\"Linear output shape: {linear_output.shape}\")\n",
        "    print(f\"Activated output shape: {activated_output.shape}\")\n",
        "    print(f\"Number of negative values before ReLU: {(linear_output < 0).sum().item()}\")\n",
        "    print(f\"Number of negative values after ReLU: {(activated_output < 0).sum().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHjrTFja2rFD",
        "outputId": "6e7d2c43-5ec4-4660-822f-1af86f5a0a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 2: Activation Functions\n",
            "==================================================\n",
            "❌ ReLU activation function: Unexpected error: Exercise3Validator.test_relu_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Sigmoid activation function: Unexpected error: Exercise3Validator.test_sigmoid_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Tanh activation function: Unexpected error: Exercise3Validator.test_tanh_activation() missing 1 required positional argument: 'variables'\n",
            "❌ Linear layer output computation: Unexpected error: Exercise3Validator.test_linear_output() missing 1 required positional argument: 'variables'\n",
            "❌ ReLU activation applied correctly: Unexpected error: Exercise3Validator.test_activated_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 2: Activation Functions - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Test Section 2: Activation Functions\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 2: Activation Functions\"]]\n",
        "test_runner.test_section(\"Section 2: Activation Functions\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geRw91N92rFD"
      },
      "source": [
        "## Section 3: Building Networks with nn.Sequential\n",
        "\n",
        "`nn.Sequential` allows us to stack layers and create a neural network pipeline. The output of each layer becomes the input to the next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTrAyrl32rFD",
        "outputId": "95953a30-b078-48e2-d7fb-5467427be142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple MLP architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=4, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Total parameters: 46\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a simple 2-layer MLP using nn.Sequential\n",
        "# Input: 8 features -> Hidden: 4 neurons with ReLU -> Output: 2 neurons\n",
        "simple_mlp = nn.Sequential(\n",
        "    nn.Linear(8, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 2),\n",
        ")\n",
        "\n",
        "if simple_mlp is not None:\n",
        "    print(\"Simple MLP architecture:\")\n",
        "    print(simple_mlp)\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in simple_mlp.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "jY6mkeSt2rFE",
        "outputId": "3d48457d-a9b7-44f0-fc8c-d2cdde3a9810",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep MLP architecture:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=10, out_features=8, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=8, out_features=6, bias=True)\n",
            "  (3): ReLU()\n",
            "  (4): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (5): ReLU()\n",
            "  (6): Linear(in_features=4, out_features=2, bias=True)\n",
            "  (7): ReLU()\n",
            ")\n",
            "\n",
            "Calculated parameters: 180\n",
            "Actual parameters: 180\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a deeper MLP with 3 hidden layers\n",
        "# Input: 10 -> Hidden1: 8 (ReLU) -> Hidden2: 6 (ReLU) -> Hidden3: 4 (ReLU) -> Output: 2\n",
        "deep_mlp = nn.Sequential(\n",
        "    nn.Linear(10, 8),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(8, 6),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(6, 4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4, 2),\n",
        "    nn.ReLU()\n",
        "\n",
        ")\n",
        "# TODO: Calculate the total number of parameters in deep_mlp\n",
        "# Parameters per layer: (input_size * output_size) + output_size\n",
        "# Layer 1: (10 * 8) + 8 = 88\n",
        "# Layer 2: (8 * 6) + 6 = 54\n",
        "# Layer 3: (6 * 4) + 4 = 28\n",
        "# Layer 4: (4 * 2) + 2 = 10\n",
        "deep_mlp_params = sum(p.numel() for p in deep_mlp.parameters()) # Calculate the sum\n",
        "\n",
        "if deep_mlp is not None and deep_mlp_params is not None:\n",
        "    print(\"Deep MLP architecture:\")\n",
        "    print(deep_mlp)\n",
        "    print(f\"\\nCalculated parameters: {deep_mlp_params}\")\n",
        "    actual_params = sum(p.numel() for p in deep_mlp.parameters())\n",
        "    print(f\"Actual parameters: {actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CqNitb3D2rFE",
        "outputId": "fb281349-9186-445c-a4d3-37a215f85b13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 3: Building Networks with nn.Sequential\n",
            "==================================================\n",
            "❌ Simple 2-layer MLP structure: Unexpected error: Exercise3Validator.test_simple_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Deep 4-layer MLP structure: Unexpected error: Exercise3Validator.test_deep_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Correct parameter count for deep_mlp: Unexpected error: Exercise3Validator.test_deep_mlp_params() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 3: Building Networks with nn.Sequential - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Test Section 3: Building Networks with nn.Sequential\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 3: Building Networks with nn.Sequential\"]]\n",
        "test_runner.test_section(\"Section 3: Building Networks with nn.Sequential\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9LcSpfU2rFE"
      },
      "source": [
        "## Section 4: Forward Pass\n",
        "\n",
        "The forward pass is the process of passing input data through the network to get predictions. Each layer transforms the data sequentially."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bDD6XZ9s2rFE",
        "outputId": "22ffd455-fe1c-4eb0-ef82-7564347d5be8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 8])\n",
            "Output shape: torch.Size([3, 2])\n",
            "Output values:\n",
            "tensor([[-0.1081,  0.0376],\n",
            "        [-0.3710,  0.1135],\n",
            "        [-0.2570,  0.1915]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Perform a forward pass through simple_mlp\n",
        "# Create input data with batch size 3 and 8 features\n",
        "forward_input = torch.randn(3, 8)\n",
        "\n",
        "# TODO: Pass the input through simple_mlp\n",
        "simple_output = simple_mlp(forward_input)\n",
        "\n",
        "if simple_output is not None:\n",
        "    print(f\"Input shape: {forward_input.shape}\")\n",
        "    print(f\"Output shape: {simple_output.shape}\")\n",
        "    print(f\"Output values:\\n{simple_output}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kHFDRT-u2rFE",
        "outputId": "d94568d9-822c-43f5-d416-439d33b61286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mixed activation MLP:\n",
            "Sequential(\n",
            "  (0): Linear(in_features=6, out_features=4, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (3): Tanh()\n",
            "  (4): Linear(in_features=3, out_features=1, bias=True)\n",
            "  (5): Sigmoid()\n",
            ")\n",
            "\n",
            "Input shape: torch.Size([5, 6])\n",
            "Output shape: torch.Size([5, 1])\n",
            "Output range: [0.4208, 0.4664]\n",
            "(Note: Sigmoid ensures output is between 0 and 1)\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a network with mixed activation functions\n",
        "# Input: 6 -> Hidden1: 4 (ReLU) -> Hidden2: 3 (Tanh) -> Output: 1 (Sigmoid)\n",
        "mixed_activation_mlp = nn.Sequential(\n",
        "    nn.Linear(6,4),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(4,3),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(3,1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# TODO: Perform forward pass with batch size 5\n",
        "mixed_input = torch.randn(5, 6)\n",
        "mixed_output = mixed_activation_mlp(mixed_input) # Pass mixed_input through mixed_activation_mlp\n",
        "\n",
        "if mixed_activation_mlp is not None and mixed_output is not None:\n",
        "    print(\"Mixed activation MLP:\")\n",
        "    print(mixed_activation_mlp)\n",
        "    print(f\"\\nInput shape: {mixed_input.shape}\")\n",
        "    print(f\"Output shape: {mixed_output.shape}\")\n",
        "    print(f\"Output range: [{mixed_output.min().item():.4f}, {mixed_output.max().item():.4f}]\")\n",
        "    print(\"(Note: Sigmoid ensures output is between 0 and 1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4I0RPPb2rFE",
        "outputId": "876cc0bb-295c-4ef5-f4fc-b6d70529d636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 4: Forward Pass\n",
            "==================================================\n",
            "❌ Forward pass through simple_mlp: Unexpected error: Exercise3Validator.test_simple_output() missing 1 required positional argument: 'variables'\n",
            "❌ MLP with mixed activations: Unexpected error: Exercise3Validator.test_mixed_activation_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Forward pass through mixed_activation_mlp: Unexpected error: Exercise3Validator.test_mixed_output() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 4: Forward Pass - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Test Section 4: Forward Pass\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 4: Forward Pass\"]]\n",
        "test_runner.test_section(\"Section 4: Forward Pass\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW6VtJjp2rFE"
      },
      "source": [
        "## Section 5: Understanding Parameter Counting\n",
        "\n",
        "Understanding how many parameters your network has is crucial for model complexity and memory requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UaLIfqMO2rFE",
        "outputId": "2494705e-2d75-4af6-9872-edbb7df67ec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple MLP parameters: 46\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a function to count parameters in any model\n",
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    Count the total number of trainable parameters in a model.\n",
        "\n",
        "    Args:\n",
        "        model: A PyTorch nn.Module\n",
        "\n",
        "    Returns:\n",
        "        Total number of parameters\n",
        "    \"\"\"\n",
        "    # TODO: Complete this function\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "# Test your function\n",
        "if count_parameters is not None and simple_mlp is not None:\n",
        "    param_count = count_parameters(simple_mlp)\n",
        "    if param_count is not None:\n",
        "        print(f\"Simple MLP parameters: {param_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "k7fGPtZZ2rFF",
        "outputId": "2d37353c-e469-49c4-e2c3-3596fd4158f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'torch.nn' has no attribute 'ReLu'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-312775162.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m large_mlp = nn.Sequential(\n\u001b[1;32m      5\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m62\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'torch.nn' has no attribute 'ReLu'"
          ]
        }
      ],
      "source": [
        "# TODO: Create a large MLP and calculate its parameters\n",
        "# Input: 100 -> Hidden1: 64 -> Hidden2: 32 -> Hidden3: 16 -> Output: 10\n",
        "# Use ReLU activation between layers (except after output)\n",
        "large_mlp = nn.Sequential(\n",
        "    nn.Linear(100,64),\n",
        "    nn.ReLu(),\n",
        "    nn.Linear(64,62),\n",
        "    nn.ReLu(),\n",
        "    nn.Linear(32,10)\n",
        ")\n",
        "\n",
        "# TODO: Calculate expected number of parameters manually\n",
        "# Layer 1: (100 * 64) + 64 = ?\n",
        "# Layer 2: (64 * 32) + 32 = ?\n",
        "# Layer 3: (32 * 16) + 16 = ?\n",
        "# Layer 4: (16 * 10) + 10 = ?\n",
        "expected_params = None  # Sum all layer parameters\n",
        "\n",
        "if large_mlp is not None and expected_params is not None and count_parameters is not None:\n",
        "    actual_params = count_parameters(large_mlp)\n",
        "    if actual_params is not None:\n",
        "        print(f\"Expected parameters: {expected_params}\")\n",
        "        print(f\"Actual parameters: {actual_params}\")\n",
        "        print(f\"Match: {expected_params == actual_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sr_Bnoe2rFF",
        "outputId": "bacbaf48-a307-448e-e2af-7a6ba43e0cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 5: Understanding Parameter Counting\n",
            "==================================================\n",
            "❌ Parameter counting function: Unexpected error: Exercise3Validator.test_count_parameters_function() missing 1 required positional argument: 'variables'\n",
            "❌ Large MLP structure: Unexpected error: Exercise3Validator.test_large_mlp() missing 1 required positional argument: 'variables'\n",
            "❌ Correct manual parameter calculation: Unexpected error: Exercise3Validator.test_expected_params() missing 1 required positional argument: 'variables'\n",
            "\n",
            "❌ Section 5: Understanding Parameter Counting - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Test Section 5: Understanding Parameter Counting\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE3_SECTIONS[\"Section 5: Understanding Parameter Counting\"]]\n",
        "test_runner.test_section(\"Section 5: Understanding Parameter Counting\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APJJavHm2rFF"
      },
      "source": [
        "## Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6AvJNen2rFF",
        "outputId": "b29acaf2-40d2-40ee-b694-e00277f5a3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL TEST SUMMARY\n",
            "============================================================\n",
            "❌ Section 1: Understanding nn.Linear: 0/3 tests passed\n",
            "❌ Section 2: Activation Functions: 0/5 tests passed\n",
            "❌ Section 3: Building Networks with nn.Sequential: 0/3 tests passed\n",
            "❌ Section 4: Forward Pass: 0/3 tests passed\n",
            "❌ Section 5: Understanding Parameter Counting: 0/3 tests passed\n",
            "============================================================\n",
            "❌ Some tests are still failing. Please review and complete the TODOs.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Display final summary of all tests\n",
        "test_runner.final_summary()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}