{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0rUxfbJuUGd"
      },
      "source": [
        "# Module 2 - Exercise 1: Autograd Exploration\n",
        "\n",
        "## Learning Objectives\n",
        "- Master PyTorch's automatic differentiation system\n",
        "- Understand computational graphs and gradient flow\n",
        "- Practice with multivariable gradients and chain rule\n",
        "- Explore gradient context management\n",
        "- Implement higher-order derivatives\n",
        "\n",
        "## Prerequisites\n",
        "- Completion of Module 1 exercises\n",
        "- Understanding of calculus derivatives\n",
        "- Familiarity with chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdVKMMPouUGf"
      },
      "source": [
        "## Setup and Test Repository\n",
        "\n",
        "First, let's clone the test repository and set up our environment for step-by-step validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhz0Z8sNuUGg",
        "outputId": "1651104c-f1a4-4231-f79d-6a9512fc2b4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test framework setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Clone the test repository\n",
        "!git clone https://github.com/racousin/data_science_practice.git /tmp/tests 2>/dev/null || true\n",
        "\n",
        "# Import required modules\n",
        "import sys\n",
        "sys.path.append('/tmp/tests/tests/python_deep_learning')\n",
        "\n",
        "# Import the improved test utilities\n",
        "from test_utils import NotebookTestRunner, create_inline_test\n",
        "from module2.test_exercise1 import Exercise1Validator, EXERCISE1_SECTIONS\n",
        "\n",
        "# Create test runner and validator\n",
        "test_runner = NotebookTestRunner(\"module2\", 1)\n",
        "validator = Exercise1Validator()\n",
        "\n",
        "print(\"Test framework setup complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_WmTnAuuUGh"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h99OS9uCuUGi",
        "outputId": "65caf137-1d48-4701-c5c8-3455d34bd7c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Print PyTorch version\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnnNb7gQuUGi"
      },
      "source": [
        "## Section 1: Basic Autograd Operations\n",
        "\n",
        "Learn the fundamentals of automatic differentiation with simple scalar functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YkypL2quUGj",
        "outputId": "10fb1847-c163-4153-a55b-d744b5c42ac2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = 2.0\n",
            "y = 11.0\n",
            "x.requires_grad: True\n",
            "y.requires_grad: True\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a tensor that requires gradients and compute a simple function\n",
        "# Create x = 2.0 with requires_grad=True\n",
        "x = torch.tensor(2.,requires_grad = True)\n",
        "\n",
        "# TODO: Compute y = x^2 + 3*x + 1\n",
        "y = x**2 +3*x+1\n",
        "\n",
        "print(f\"x = {x}\")\n",
        "print(f\"y = {y}\")\n",
        "print(f\"x.requires_grad: {x.requires_grad}\")\n",
        "print(f\"y.requires_grad: {y.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suyq82fkuUGj",
        "outputId": "d4645382-d74a-4fe0-c235-110692e39efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dy/dx = 7.0\n",
            "Expected: dy/dx = 2x + 3 = 2*2 + 3 = 7\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute gradients using backward()\n",
        "# Call y.backward() to compute gradients\n",
        "y.backward()\n",
        "print(f\"dy/dx = {x.grad}\")\n",
        "print(f\"Expected: dy/dx = 2x + 3 = 2*2 + 3 = 7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwj3PFp3uUGj",
        "outputId": "05c830cc-bd2f-431f-c360-b960a96eb421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 1: Basic Autograd Operations\n",
            "==================================================\n",
            "❌ Create x = 2.0 with requires_grad=True: Unexpected error: Exercise1Validator.test_x() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute y = x^2 + 3*x + 1: Unexpected error: Exercise1Validator.test_y() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute gradient dy/dx using backward(): Unexpected error: Exercise1Validator.test_x_grad() missing 1 required positional argument: 'namespace'\n",
            "\n",
            "❌ Section 1: Basic Autograd Operations - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# Test Section 1: Basic Autograd Operations\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 1: Basic Autograd Operations\"]]\n",
        "test_runner.test_section(\"Section 1: Basic Autograd Operations\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHiALGfLuUGk"
      },
      "source": [
        "## Section 2: Multivariable Gradients\n",
        "\n",
        "Explore gradients with functions of multiple variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6ajm-W8uUGk",
        "outputId": "f6de242e-1856-4bdb-d911-ffc416253bdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x1 = 1.0, x2 = 2.0\n",
            "z = 11.0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create two variables with gradients enabled x1 = 1.0, x2 = 2.0\n",
        "x1 = torch.tensor(1.,requires_grad= True)\n",
        "x2 = torch.tensor(2.,requires_grad= True)\n",
        "\n",
        "# TODO: Compute z = x1^2 + x2^3 + x1*x2\n",
        "z = x1**2 + x2**3 + x1*x2\n",
        "\n",
        "print(f\"x1 = {x1}, x2 = {x2}\")\n",
        "print(f\"z = {z}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv-Tt8-quUGk",
        "outputId": "4a0384af-7fa1-409d-a1a7-22d1062748c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "∂z/∂x1 = 4.0\n",
            "∂z/∂x2 = 13.0\n",
            "Expected: ∂z/∂x1 = 2*x1 + x2 = 2*1 + 2 = 4\n",
            "Expected: ∂z/∂x2 = 3*x2^2 + x1 = 3*4 + 1 = 13\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute gradients for multivariable function\n",
        "# Call z.backward() to compute partial derivatives\n",
        "z.backward()\n",
        "print(f\"∂z/∂x1 = {x1.grad}\")\n",
        "print(f\"∂z/∂x2 = {x2.grad}\")\n",
        "print(f\"Expected: ∂z/∂x1 = 2*x1 + x2 = 2*1 + 2 = 4\")\n",
        "print(f\"Expected: ∂z/∂x2 = 3*x2^2 + x1 = 3*4 + 1 = 13\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxF7TVbZuUGl",
        "outputId": "d48701c6-3de4-48bd-d82a-3f607f1d33d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 2: Multivariable Gradients\n",
            "==================================================\n",
            "❌ Create x1 = 1.0 with requires_grad=True: Unexpected error: Exercise1Validator.test_x1() missing 1 required positional argument: 'namespace'\n",
            "❌ Create x2 = 2.0 with requires_grad=True: Unexpected error: Exercise1Validator.test_x2() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute z = x1^2 + x2^3 + x1*x2: Unexpected error: Exercise1Validator.test_z() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute partial derivative ∂z/∂x1: Unexpected error: Exercise1Validator.test_x1_grad() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute partial derivative ∂z/∂x2: Unexpected error: Exercise1Validator.test_x2_grad() missing 1 required positional argument: 'namespace'\n",
            "\n",
            "❌ Section 2: Multivariable Gradients - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "# Test Section 2: Multivariable Gradients\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 2: Multivariable Gradients\"]]\n",
        "test_runner.test_section(\"Section 2: Multivariable Gradients\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdkN50qKuUGl"
      },
      "source": [
        "## Section 3: Vector and Matrix Gradients\n",
        "\n",
        "Work with gradients of vector and matrix operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tI1PAxBLuUGl",
        "outputId": "00b57a36-1fd7-4a55-8334-95bdf2ed3ee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vec_x = tensor([1., 2., 3.], requires_grad=True)\n",
            "vec_sum = 14.0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a vector [1.0, 2.0, 3.0] with gradients and compute a scalar loss\n",
        "vec_x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
        "\n",
        "# TODO: Compute sum of squares sum(vec_x**2)\n",
        "vec_sum = (vec_x**2).sum()\n",
        "\n",
        "print(f\"vec_x = {vec_x}\")\n",
        "print(f\"vec_sum = {vec_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb_b_bCAuUGl",
        "outputId": "7403cc62-f952-492a-af2d-57656f9c3594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "∇vec_sum = tensor([2., 4., 6.])\n",
            "Expected: gradient should be 2*vec_x = [2, 4, 6]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute gradients for vector function\n",
        "# Call vec_sum.backward()\n",
        "vec_sum.backward()\n",
        "print(f\"∇vec_sum = {vec_x.grad}\")\n",
        "print(f\"Expected: gradient should be 2*vec_x = [2, 4, 6]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_DzUuuWuUGm",
        "outputId": "a46dc4d8-5928-45eb-ae54-14a59d838cba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 3a: Vector Gradients\n",
            "==================================================\n",
            "❌ Create vector [1.0, 2.0, 3.0] with requires_grad=True: Unexpected error: Exercise1Validator.test_vec_x() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute vec_sum = sum of squares: Unexpected error: Exercise1Validator.test_vec_sum() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute gradient for vector: Unexpected error: Exercise1Validator.test_vec_x_grad() missing 1 required positional argument: 'namespace'\n",
            "\n",
            "❌ Section 3a: Vector Gradients - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# Test vector gradients (first part of Section 3)\n",
        "vec_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][:3]]\n",
        "test_runner.test_section(\"Section 3a: Vector Gradients\", validator, vec_tests, locals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjZjcEvTuUGm",
        "outputId": "3787e238-437e-4373-a00b-339279c84e7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mat_A = \n",
            "tensor([[1., 2.],\n",
            "        [3., 4.]], requires_grad=True)\n",
            "mat_sum = 30.0\n"
          ]
        }
      ],
      "source": [
        "# TODO: Create a matrix [[1.0, 2.0], [3.0, 4.0]] and compute gradients\n",
        "mat_A = torch.tensor( [[1.0, 2.0], [3.0, 4.0]],requires_grad=True)\n",
        "\n",
        "# TODO: Compute mat_sum = sum of squares of all elements\n",
        "mat_sum = torch.sum(mat_A**2)  # torch.sum(mat_A**2)\n",
        "\n",
        "print(f\"mat_A = \\n{mat_A}\")\n",
        "print(f\"mat_sum = {mat_sum}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGtXlQNkuUGm",
        "outputId": "96dfa895-e84f-4346-cb4a-9aaeceab2619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "∇mat_A = \n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "Expected: gradient should be 2*mat_A\n"
          ]
        }
      ],
      "source": [
        "# TODO: Compute matrix gradients\n",
        "# Call mat_sum.backward()\n",
        "mat_sum.backward()\n",
        "print(f\"∇mat_A = \\n{mat_A.grad}\")\n",
        "print(f\"Expected: gradient should be 2*mat_A\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC9QSQgiuUGn",
        "outputId": "f1412f90-23a1-4f77-8304-c33ae09167b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Testing: Section 3b: Matrix Gradients\n",
            "==================================================\n",
            "❌ Create 2x2 matrix [[1, 2], [3, 4]] with requires_grad=True: Unexpected error: Exercise1Validator.test_mat_A() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute mat_sum = sum of squares: Unexpected error: Exercise1Validator.test_mat_sum() missing 1 required positional argument: 'namespace'\n",
            "❌ Compute gradient for matrix: Unexpected error: Exercise1Validator.test_mat_A_grad() missing 1 required positional argument: 'namespace'\n",
            "\n",
            "❌ Section 3b: Matrix Gradients - Some tests failed. Review the errors above.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# Test matrix gradients (second part of Section 3)\n",
        "mat_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 3: Vector and Matrix Gradients\"][3:]]\n",
        "test_runner.test_section(\"Section 3b: Matrix Gradients\", validator, mat_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDG1e6RYuUGn"
      },
      "source": [
        "## Section 4: Computational Graph and Chain Rule\n",
        "\n",
        "Understand how PyTorch builds and traverses computational graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0plIBrDuUGn",
        "outputId": "ab0a797c-4407-4fe7-8b25-2c9e986e4ea9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.12/dist-packages (0.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from torchviz) (2.8.0+cu126)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from torchviz) (0.21)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->torchviz) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->torchviz) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->torchviz) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchviz # uncomment if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "uDm8NoVGuUGn",
        "outputId": "f1029919-25fb-4f30-b8f0-6fbabf1d1970"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch.viz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2091024755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviz\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m \u001b[0;31m# display with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# TODO: Build a computational graph step by step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# torch.tensor(2.0, requires_grad=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"x = {x}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.viz'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from torch.viz import make_dot # display with\n",
        "# TODO: Build a computational graph step by step\n",
        "x = torch.tensor(2.0, requires_grad=True)  # torch.tensor(2.0, requires_grad=True)\n",
        "print(f\"x = {x}\")\n",
        "dot = make_dot(x, params={\"x\": x})\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7kbMCNBuUGo"
      },
      "outputs": [],
      "source": [
        "# TODO: Build computation step by step\n",
        "y = None  # xˆ2\n",
        "print(f\"y = x^2 = {y}\")\n",
        "dot = make_dot(y, params={\"x\": x})\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf6Q0ZIAuUGo"
      },
      "outputs": [],
      "source": [
        "z = None  # 3y + 1\n",
        "print(f\"z = 3y + 1 = {z}\")\n",
        "dot = make_dot(z, params={\"x\": x})\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48yhYVhVuUGo"
      },
      "outputs": [],
      "source": [
        "w = None  # z^2\n",
        "print(f\"w = z^2 = {w}\")\n",
        "dot = make_dot(w, params={\"x\": x})\n",
        "dot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9tvvj_cuUGp"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute gradients through the computational graph\n",
        "# Call graph_w.backward()\n",
        "\n",
        "print(f\"dw/dx = {graph_x.grad}\")\n",
        "print(f\"Chain rule: dw/dx = dw/dz * dz/dy * dy/dx\")\n",
        "print(f\"dw/dz = 2*z = 2*13 = 26\")\n",
        "print(f\"dz/dy = 3\")\n",
        "print(f\"dy/dx = 2*x = 2*2 = 4\")\n",
        "print(f\"Therefore: dw/dx = 26 * 3 * 4 = 312\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23hEcjvKuUGp"
      },
      "source": [
        "## Section 5: Gradient Context Management\n",
        "\n",
        "Learn to control when gradients are computed and stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfzKBt-yuUGp"
      },
      "outputs": [],
      "source": [
        "# TODO: Use torch.no_grad() context\n",
        "x = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# TODO: Compute operation within no_grad context\n",
        "with torch.no_grad():\n",
        "    no_grad_result = None  # x**2 + 2*x\n",
        "\n",
        "print(f\"no_grad_result = {no_grad_result}\")\n",
        "print(f\"requires_grad: {no_grad_result.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1QK0NN8uUGp"
      },
      "outputs": [],
      "source": [
        "# TODO: Use .detach() to remove tensor from computational graph\n",
        "y = x**3 + x\n",
        "y_detached_result = None\n",
        "\n",
        "print(f\"Original y requires_grad: {y.requires_grad}\")\n",
        "print(f\"Detached result requires_grad: {y_detached_result.requires_grad}\")\n",
        "print(f\"Values are equal: {torch.equal(y, y_detached_result)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-djEGPpkuUGq"
      },
      "outputs": [],
      "source": [
        "# Test Section 5: Gradient Context Management\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 5: Gradient Context Management\"]]\n",
        "test_runner.test_section(\"Section 5: Gradient Context Management\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7g_UU_uUGq"
      },
      "source": [
        "## Section 6: Higher-Order Derivatives\n",
        "\n",
        "Compute second derivatives and higher-order gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ACl2pVQuUGr"
      },
      "outputs": [],
      "source": [
        "# TODO: Compute second derivative\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# TODO: Define function f(x) = x^4\n",
        "y = None\n",
        "\n",
        "# TODO: Compute first derivative\n",
        "# y.backward(create_graph=True)  # create_graph=True allows computing gradients of gradients\n",
        "# first_derivative = x.grad.clone()\n",
        "\n",
        "print(f\"f(x) = x^4, x = {x}\")\n",
        "# print(f\"f'(x) = 4x^3 = {first_derivative}\")\n",
        "\n",
        "# TODO: Compute second derivative\n",
        "# x.grad.zero_()  # Clear first derivative\n",
        "# first_derivative.backward()\n",
        "second_derivative = None  # x.grad\n",
        "\n",
        "print(f\"f''(x) = 12x^2 = {second_derivative}\")\n",
        "print(f\"Expected: f''(2) = 12*4 = 48\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70H76Q5buUGr"
      },
      "outputs": [],
      "source": [
        "# Test Section 6: Higher-Order Derivatives\n",
        "section_tests = [(getattr(validator, name), desc) for name, desc in EXERCISE1_SECTIONS[\"Section 6: Higher-Order Derivatives\"]]\n",
        "test_runner.test_section(\"Section 6: Higher-Order Derivatives\", validator, section_tests, locals())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMGsCdvMuUGr"
      },
      "source": [
        "## Section 7: Gradient Flow Visualization\n",
        "\n",
        "Visualize how gradients flow through computational graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Sm2pXr9uUGr"
      },
      "outputs": [],
      "source": [
        "# Create a more complex computational graph for visualization\n",
        "def create_complex_function(x):\n",
        "    \"\"\"Create a complex function for gradient flow analysis\"\"\"\n",
        "    a = x**2\n",
        "    b = torch.sin(a)\n",
        "    c = torch.exp(b)\n",
        "    d = torch.log(c + 1)\n",
        "    return d\n",
        "\n",
        "# Test with different input values\n",
        "x_values = torch.linspace(-2, 2, 100)\n",
        "gradients = []\n",
        "\n",
        "for x_val in x_values:\n",
        "    x = torch.tensor(x_val.item(), requires_grad=True)\n",
        "    y = create_complex_function(x)\n",
        "    y.backward()\n",
        "    gradients.append(x.grad.item())\n",
        "\n",
        "# Plot function and its gradient\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "with torch.no_grad():\n",
        "    y_values = [create_complex_function(x).item() for x in x_values]\n",
        "plt.plot(x_values.numpy(), y_values)\n",
        "plt.title('Function: log(exp(sin(x²)) + 1)')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(x_values.numpy(), gradients)\n",
        "plt.title(\"Function's Gradient\")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel(\"f'(x)\")\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"The gradient plot shows how the derivative changes across the input domain.\")\n",
        "print(\"Notice how the gradient reflects the slope of the original function.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}